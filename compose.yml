x-logging: &default-logging
  driver: json-file
  options:
    max-size: "50m"
    max-file: "3"

services:
  kafka:
    logging: *default-logging
    image: apache/kafka:latest
    ports:
      - "9094:9094"
    environment:
      # core KRaft / node
      - KAFKA_NODE_ID=0
      - KAFKA_PROCESS_ROLES=controller,broker
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CONTROLLER_QUORUM_VOTERS=0@kafka:9093

      # listeners
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      # broker behavior
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=false

      # single-broker REQUIRED internals
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1

      # retention / log dirs
      - KAFKA_LOG_RETENTION_BYTES=3221225472
      - KAFKA_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_LOG_CLEANUP_POLICY=delete
      - KAFKA_LOG_DIRS=/var/lib/kafka/data
    volumes:
      - kafka-data:/var/lib/kafka/data

  kafka-admin:
    build: 
      context: './services/admin'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock 
    restart: "no"
  
  kafka-ui:
    logging: *default-logging
    image: 'provectuslabs/kafka-ui:latest'
    ports:
      - '8080:8080'
    depends_on:
      - kafka
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092 
  
  logger:
    logging: *default-logging
    build: 
        context: './services/logger'
        dockerfile: Dockerfile
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - logger:/app/output
    depends_on:
        - kafka

  selenium-hub:
    logging: *default-logging
    image: selenium/hub:latest
    container_name: selenium-hub
    ports:
      - "4442:4442"
      - "4443:4443"
      - "4444:4444"

  firefox:
    logging: *default-logging
    image: selenium/node-firefox:latest
    depends_on:
      - selenium-hub
    environment:
      # tell the node where the hub lives
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443

      # override default max sessions
      - SE_NODE_OVERRIDE_MAX_SESSIONS=true
      - SE_NODE_MAX_SESSIONS=2
      - SE_NODE_MAX_INSTANCES=2
      - SE_NODE_SESSION_TIMEOUT=300000000
    shm_size: 2g            # required for Firefox stability
    mem_limit: 1g           # hard memory cap
    mem_reservation: 1g  # soft memory cap

  scraper:
    logging: *default-logging
    build: 
        context: './services/scraper'
        dockerfile: Dockerfile
    environment:
      - CRYPTO_PANIC_KEY=${CRYPTO_PANIC_KEY}
      - BOOTSTRAP_SERVERS=kafka:9092
      - SELENIUM_URL=http://selenium-hub:4444/wd/hub
      - QUEUE_INIT_TIME=3600000
    volumes:
      - scraper:/app/db
    depends_on:
        - kafka

  backend:
    build: 
      context: './services/backend'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    ports:
      - '7979:7979' 
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092

  sentiment-gemini:
    logging: *default-logging
    build: 
      context: './services/llm'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    environment:
      - API_KEY=${GEMINI_API_KEY}
      - HTTP_ENDPOINT=${GEMINI_ENDPOINT}
      - MODEL=${GEMINI_MODEL}
      - KAFKA_BROKER=kafka:9092
      - INPUT_TOPIC=events
      - OUTPUT_TOPIC=sentiment-scores
      - GROUP_ID=gemini
      - MIN_BATCH_SIZE=2
      - MAX_BATCH_SIZE=4
      - FLUSH_INTERVAL=180
      - MAX_RETRIES=5
      - MAX_RPM=12
      - MAX_RPD=960
    
  sentiment-nlp:
    logging: *default-logging
    build: 
      context: './services/nlp'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:9092
      - INPUT_TOPIC=events
      - OUTPUT_TOPIC=sentiment-scores
      - GROUP_ID=nlp

  sentiment-deepseek:
    logging: *default-logging
    build: 
      context: './services/llm'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    environment:
      - HTTP_ENDPOINT=${DEEPSEEK_ENDPOINT}
      - MODEL=${DEEPSEEK_MODEL}
      - WOL_ENDPOINT=${WOL_ENDPOINT}
      - KAFKA_BROKER=kafka:9092
      - INPUT_TOPIC=events
      - OUTPUT_TOPIC=sentiment-scores
      - GROUP_ID=deepseek
      - MIN_BATCH_SIZE=2
      - MAX_BATCH_SIZE=2
      - FLUSH_INTERVAL=30
      - MAX_RETRIES=2
      - MAX_RPM=6969696960
      - MAX_RPD=6969696960
    
  model:
    build: 
      context: './services/model'
      dockerfile: Dockerfile
    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:9092
      - INPUT_TOPIC=sentiment-scores
      - OUTPUT_TOPIC=model-params
      - DECAYED_SCORES_TOPIC=model-decayed-scores
      - GROUP_ID=model
      - PREDICTION_OUTPUT_TOPIC=model-predictions


volumes:
  csv_data:
  logger:
  scraper:
  kafka-data:
    name: kafka-data

